---
format: pdf
output:
  pdf_document:
    latex_engine: pdflatex
    toc: true
header-includes:
  \usepackage{graphicx}
  \usepackage{grffile}
  \usepackage{xcolor}
  \usepackage{fancyhdr}
  \usepackage{amsmath}
  \usepackage{fvextra}
  \definecolor{BERKELEYBLUE}{cmyk}{100,71,10,47}
  \definecolor{CALIFORNIAGOLD}{cmyk}{0,32,100,0}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
---

<!---
 Title Page Layout 
--->
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\topmargin1mm
\begin{center}
\huge\textbf{Genetic Algorithm for Variable Selection}\par
\vspace{1.75cm}
\huge{STAT 243 - Final Project}\par
\huge\textit{Fall 2023}\par
\vspace{3cm}
\large \textbf{Authors: Sean Zhou, Sun Moon, Frederik Stihler} \par
\large Department of Statistics\par
Instructor: Christopher Paciorek
\par 
\par 
\vspace{1cm}
\large December 13th, 2023\par
\vspace{4cm}
\includegraphics[width=0.4\textwidth]{Berkeley_logo.svg.png} %70% der Textbreite
\end{center}
\thispagestyle{empty}
\newpage
\fancyfoot[C]{\thepage}

# 1. Introduction

The goal of this project is to develop a genetic algorithm for variable selection, including both linear regression and GLMs. The results are consolidated in a Python package. The user can input a dataset (with covariates and corresponding responses), as well as the desired type of regression. The algortihm will perform the variable selection and tell the user which features to use.

In general, variable selection is the process of selecting a subset of relevant predictors for the model creation. Variable selection generally decreases the model complexity, which can make it easier to interpret and also reduce computational complexity.

Genetic algorithms (GA) are stochastic methods usually used for optimization or search problems. They utilize principles from biological evolution and natural selection, such as selection, crossover and mutation.


# 2. Programming approach

## 2.1 Overall structure

We identified the following main steps in the algorithm:

1. Population initialization
2. Evolution cycle for fixed number of iterations
    - Fitness assessment and ranking
    - Parent selection
    - Genetic operators
        - Crossover
        - Mutation
        - etc.
3. Ouput fittest individual of final population

Each (sub-)step was implemented as a modular component. We decided to use an object-oriented programming approach, which means that the individual steps are implemented as Python class methods. To keep the code organized in a logical order we  applied inheritance so that each step is defined in its own class. Then, these classes and methods are inherited by the parent class "GA", which has a primary method "select" that carries out the overall algorithm outlined above.

## 2.2 Default parameter values and initialization

Based on the theory of Givens and Hoeting and the examples outlined below, we have decided on certain default parameters. 

The theory suggests a population size P in the range of $C < P < 2C$, where C is the number of predictors (i.e. number of genes per chromosome). We chose the midpoint of this range as default value if no specific population size is given by the user: $P = 1.5 \times C$. As the population size should remain constant during the evolution of the algorithm and due to the crossover operation requiring an even number of chromosomes, an additional chromosome is added if the population size is odd (the user is informed about this modification). The initial population is fully randomized (random binary choice for all loci of all chromosomes).

The default genetic operators applied per generation are single split crossover and standard mutation. However, the user can also provide his own genetic operators (implemented on the full population) or modify the order of execution. In addition to the default crossover, a version with random allel selection from the parents is included in the package. The commonly used mutation probability of 1% is set as the default. 

Lastly, our algorithm does not process the input data (no normalization or scaling of the data). This is something that user needs to do in advance.

# 3. Testing
The overall genetic algorithm was tested using simulated and actual datasets, including running OLS and logit models from the "How Couples Meet" and "Stay Together" survey as well as the baseball data set provided by Givens and Hoeting. The individual compoment functions of the algorithm were also tested in unit tests (see pytest test suite for details).

# 4. Results

## 4.1 Example 1: Baseball data
We first run the algorithm for the baseball salaries dataset. 

\begin{center}
\includegraphics[width=0.7\textwidth]{GA_results_plot_baseball.png}
\end{center}

One hundred generations of size 26 were used. The data set includes 27 predictors for the response variable "salary", so the individual chromosomes consist of 27 binary genes. The genetic operators applied were simple split crossover and a mutation rate of 1% applied independently for each chromosome and each gene position. A clear evolution or "survival of the fittest" can be observed in figure 1. The AIC of the best model was found to be around 5373.6.

## 4.2 Example 2: Synthetic data
We also explored the algorithms performance on a simple simulated dataset. We draw samples from five independent uniform random variables $X_1, \cdots, X_5 \sim Unif(0,1)$. We then construct the response variable as: $y = 3 \times X_1 + 2 \times X_2 + \epsilon$, where $\epsilon$ is Gaussian noise. In this straightforward example, the GA identified predictors $X_1$ and $X_2$ as the only relevant features after 7 iterations using the default parametrization outlined above.


## 4.3 Comparison with Lasso
Finally, the performance of the algorithm was compared to a Lasso regression (using the statsmodel package with regularization parameter $\alpha=0.1$). The Lasso method can perform variable selection by setting some of the coefficients to zero, which will "deselect" unimportant features.

\begin{center}
\includegraphics[width=0.7\textwidth]{Lasso_comparison.png}
\end{center}

The GA selected 10 predictors for the regression, meaning that 17 features where ignored. Comparing this to the coefficient sizes of the Lasso shows that similar features are considered relevant by the two models. In figure 2, the size of the horizontal bars indicates the importance of each feature (corresponding to the respective coefficient weights).

# 5. Contributions

*Sun Moon* 
Sun dedicated his efforts to the testing phase, crafting a comprehensive suite of tests to validate the algorithm. His contributions included developing test examples for the overall algorithm and implementing unit tests for individual functions using pytest. Additionally, Sun came up with testing examples, offering valuable insights into the algorithm's performance.

*Sean Zhou*
Sean was responsible for structuring and developing core components of the algorithm. His tasks included for example working on the overall GA class, the primary function "select", and parent selection. Moreover, Sean designed the python package structure and managed the group's GitHub repository, ensuring effective collaboration and organization.

*Frederik Stihler*
Frederik was responsible for structuring and developing core components of the algorithm. He focused for example on essential functions within the algorithm's structure, such as mutation, crossover, and fitness calculation. In addtion, Frederik took charge of overseeing the creation of the final report, including the examples and comparison with Lasso, ensuring clarity and coherence.

Finally, every team member performed code reviews of the other peers.


\newpage
GitHub organization: github.com \par
GitHub repository: https://github.com/seanzhou1207/Genetic-Algorithm

